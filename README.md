# SYCL-BLAS, a SYCL Basic Linear Algebra Library 

## Table of Contents

  * [Project Description](#project-description)
  * [Project Requirements](#project-requirements)
  * [Supported Platforms](#supported-platforms)
  * [Getting Started with SYCL-BLAS](#getting-started-with-sycl-blas)
  * [Support](#support)
  * [Basic SYCL-BLAS Concepts](#basic-sycl-blas-concepts)
  * [API description](#api-description)
  * [Tests and benchmarks](#tests-and-benchmarks)
  * [CMake options](#cmake-options)
  * [Cross-Compile](#cross-compile)
  * [Tests and benchmarks](#tests-and-benchmarks)
  * [Contributions](#contributions)

## Project Description

The SYCL-BLAS library is an implementation of the 
[Basic Linear Algebra Subroutines](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprogram) - using [SYCL 1.2.1](
https://www.khronos.org/registry/sycl/specs/sycl-1.2.1.pdf) and is written 
using C++11 features.

BLAS operations are being used to solve many complex problems in a range of areas 
including science, imaging and AI.

BLAS is always the lowest level in the hierarchy of numerical libraries, such that
a good BLAS implementation improves the performances of all the other
libraries.

The SYCL-BLAS project is used by the [SYCL-DNN project](https://github.com/codeplaysoftware/SYCL-DNN) 
as a supporting library to perform some operations. SYCL-DNN is a library 
implementing neural network operations.

SYCL-BLAS is designed to work with any SYCL 1.2.1 implementation. The project is developed 
using [ComputeCpp](https://developer.codeplay.com) on Ubuntu 16.04 using Intel CPU and Intel GPU.
 A BLAS library, such as OpenBLAS, is also required by the project to verify the test results.

SYCL-BLAS has been developed in collaboration with the 
*High Performance Computing & Architectures (HPCA) group*
from the Universitat Jaume I [UJI](http://www.hpca.uji.es/).

The project is maintained by [Codeplay Software](https://developer.codeplay.com).

## Supported Platforms

The master branch of SYCL-BLAS is regularly tested with the "Supported" hardware 
listed on [the ComputeCpp Supported Platforms page](https://developer.codeplay.com/products/computecpp/ce/guides/platform-support).

SYCL-BLAS may also work on other hardware and platforms assuming they implement 
 SPIR or SPIR-V support.

## Getting Started with SYCL-BLAS

### Pre-requisites

* CMake (version 3.4.2 and above)
* OpenCL 1.2-capable hardware and drivers with SPIR 1.2 or SPIR-V support
* OpenCL ICD Loader
* OpenCL headers
* gcc (version 5.4 and above)
* [ComputeCpp](https://developer.codeplay.com)

### Building SYCL-BLAS

First get OpenBLAS, since this is required to verify the test results. Instructions 
on building and installing this are on the [project website](https://www.openblas.net/).

```bash
git clone https://github.com/xianyi/OpenBLAS.git
make
make install
```

SYCL-BLAS uses CMake as its build system. All the configuration options 
can be found in the main CMakeLists.txt for the project and will show 
up in CMake GUI if you use it.

You will need to provide the location of the ComputeCpp install you are
using with the variable `ComputeCpp_DIR`. It should point to the folder
where `bin/`, `lib/` etc. are. This should be the only argument that is
mandatory, everything else should be optional. The default build type is
Release, though this can be overridden.

The following command shows how to compile SYCL-BLAS. If you have installed OpenBLAS 
in a different location adapt the path

* set `ComputeCpp_DIR` to the ComputeCpp root path

You may also need to tell CMake where to find the OpenBLAS binaries using the 
```-DCMAKE_PREFIX_PATH=/opt/OpenBLAS``` argument

```bash
git clone --recursive  https://github.com/codeplaysoftware/sycl-blas.git
mkdir build && cd build
cmake ../ -DComputeCpp_DIR=/path/to/computecpp
make
```

To install the SYCL-BLAS library

```bash
make install
```

Doxygen documentation can be generated by running:

```bash
doxygen doc/Doxyfile
```

### Sample Code

The "samples" directory contains some sample code using SYCL-BLAS 
 to implement GEMM and GEMV operations.

#### How to compile and run the samples

The "samples" folder contains a basic CMake configuration file and a module to find
SYCL-BLAS (which will be used as a header-only framework).

* set `ComputeCpp_DIR` to the ComputeCpp root path
* set `SyclBLAS_DIR` to the SYCL-BLAS project root path

```bash
mkdir build
cd build
cmake .. -DComputeCpp_DIR=/path/to/computecpp \
                 -DSyclBLAS_DIR=/path/to/syclblas
make
```

Execute the samples using the following commands
```bash
./gemm
```

or

```bash
./gemv
```

## Support

### Bug reports and Issues

Bug reports are vital to provide feedback to the developers about what is going
wrong with the project, you can raise these using the ["Issues"](https://github.com/codeplaysoftware/SYCL-BLAS/issues) 
feature in GitHub.

Please make sure that your bug report contains the following information:

* A clear and descriptive title.

* The output of
  `clinfo | grep -E "Platform ID|Name|Vendor|[Vv]ersion|Profile|Extensions"`.

* The output of `computecpp_info`.

* The exact steps and commands to run to reproduce the bug.

* The exact error text shown (if applicable), otherwise the behavior you
  expected and what you encountered instead.

* Should the problem arise outside the project's test suite then please provide
  a minimal test to allow us to reproduce the problem.

## Basic SYCL-BLAS Concepts

SYCL-BLAS uses C++ Expression Tree templates to generate SYCL Kernels via
kernel composition.
Expression Tree templates are a widely used technique to implement expressions
on C++, that facilitate development and composition of operations.
In particular,
[Kernel composition in SYCL](http://dl.acm.org/citation.cfm?id=2791332) has
been used in various projects to create efficient domain-specific embedded
languages that enable users to easily fuse GPU kernels.

SYCL-BLAS can be used
- either as a header-only framework by including sycl_blas.hpp in
an application and passing src as pass to the application include directory.
- or as a library by including sycl_blas.h in an application.

All the relevant files can be found in
the include directory.
There are four components in SYCL-BLAS, the *View*, the *Operations*,
the *Executors* and the *Interface* itself.

### Views

The input data to all the operations in SYCL-BLAS is passed to the library
using *Views*.
A *View* represents data on top of a container, passed by reference.
Views *do not store data*, they only map a visualization of the data on top
of a container.
This enables the library to implement the different indexing modes of the
BLAS API, such as strides.
Note that a view can be of a different size than a container.

All views derive from the base view class or the base matrix view class, which
represents a view of a container as a vector or as a matrix.
The container does not need to be multi-dimensional to store a matrix.
The current restriction is that container must obey the *RandomAccessIterator*
properties of the C++11 standard.

### Operations

Operations among elements of vectors (or matrices) are expressed in the
set of Operation Classes.
Operations are templated classes that take templated types as input.
Operations form the nodes of the SYCL-BLAS expression tree.
Refer to the documentation of each node type for details.

Composing these is how the compile-time Expression tree is created:
Given an operation node, the leaves of the node are other Operations.
The leaf nodes of an Expression Tree are Views or Scalar types (data).
The intermediate nodes of the Expression Tree are operations (e.g,
binary operations, unary operations, etc).

### Executors

An executor traverses the Expression Tree to evaluate the operations that it
defines.
Executors use different techniques to evaluate the expression tree.
The basic C++ executor performs a for loop on the size of the data and calls
the evaluation function on each item.

The SYCL evaluator transform the tree into a device tree (i.e, converting
buffer to accessors) and then evaluates the Expression Tree on the device.

### Interface

The different headers on the interface directory implement the traditional
BLAS interface.
Files are organised per BLAS level (1, 2, 3).

When the SYCL-BLAS BLAS interface is called, the Expression Tree for each
operation is constructed, and then executed.
Some API calls may execute several kernels (e.g, when a reduction is required).
The expression trees in the API allow to compile-time fuse operations.

Note that, although this library features a BLAS interface, users are allowed
to directly compose their own expression trees to compose multiple operations.
The CG example shows an implementation of the Conjugate Gradient that uses
various expression tree to demonstrate how to achieve compile-time kernel fusion
of multiple BLAS operations.

## API description

This section references all the supported operations and their interface.

All operations take as their first argument a reference to the executor, a
`blas::Executor` created with a `sycl::queue`. The return value is usually an
array of SYCL events (except for some operations that can return a scalar or
a tuple). The containers for the vectors and matrices (and scalars written by
the BLAS operations) are iterator buffers that can be created with
`make_sycl_iterator_buffer`.

We recommend checking the [samples](samples) to get started with SYCL-BLAS. It
is better to be familiar with BLAS:
[Wikipedia](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) ;
[Netlib reference](http://www.netlib.org/lapack/explore-html/d1/df9/group__blas.html).

### BLAS 1

The following table sums up the interface that can be found in
[blas1_interface.h](include/interface/blas1_interface.h).

For all these operations:

* `vx` and `vy` are containers for vectors `x` and `y`.
* `incx` and `incy` are their increments (number of steps to jump to the next
   value, 1 for contiguous values).
* `N`, an integer, is the size of the vectors (less than or equal to the size of
  the containers).
* `alpha` is a scalar.
* `rs` is a container of size 1, containing either a scalar, an integer, or an
  index-value tuple.
* `c` and `s` for `_rot` are scalars (cosine and sine)

| operation | arguments | description |
|---|---|---|
| `_axpy` | `ex`, `N`, `alpha`, `vx`, `incx`, `vy`, `incy` | Vector multiply-add: `y = alpha * x + y` |
| `_copy`  | `ex`, `N`, `vx`, `incx`, `vy`, `incy` | Copies a vector to another: `y = x` |
| `_dot` | `ex`, `N`, `vx`, `incx`, `vy`, `incy` [, `rs`] | Dot product of two vectors `x` and `y`; written in `rs` if passed, else returned |
| `_asum` | `ex`, `N`, `vx`, `incx` [, `rs`] | Absolute sum of the vector `x`; written in `rs` if passed, else returned |
| `_iamax` | `ex`, `N`, `vx`, `incx` [, `rs`] | First index and value of the maximum element of `x`; written in `rs` if passed, else the index only is returned |
| `_iamin` | `ex`, `N`, `vx`, `incx` [, `rs`] | First index and value of the minimum element of `x`; written in `rs` if passed, else the index only is returned |
| `_swap`  | `ex`, `N`, `vx`, `incx`, `vy`, `incy` | Interchanges two vectors: `y = x` and `x = y` |
| `_scal` | `ex`, `N`, `alpha`, `vx`, `incx` | Scalar product of a vector: `x = alpha * x` |
| `_nrm2` | `ex`, `N`, `vx`, `incx` [, `rs`] | Euclidean norm of the vector `x`; written in `rs` if passed, else returned |
| `_rot` | `ex`, `N`, `vx`, `incx`, `vy`, `incy`, `c`, `s` | Applies a plane rotation to `x` and `y` with a cosine `c` and a sine `s`  |

### BLAS 2

The following table sums up the interface that can be found in
[blas2_interface.h](include/interface/blas2_interface.h).

For all these operations:

* `trans` is a `char` representing the transpose mode of the matrix: `'n'`,
  `'t'`, or `'c'`; respectively identity, tranpose and Hermitian transpose
  (note: the latter is not relevant yet as complex numbers are not supported).
* `uplo` is a `char` that provides information about triangular matrices: `u` for
  upper triangular and `l` for lower triangular matrices.
* `diag` is a `char` that provides information about the diagonal elements of a
  triangular matrix: `u` if the matrix is unit triangular (all diagonal elements
  are 1), else `n`.
* `M` and `N` are the numbers of rows and columns of the matrix. They also
  determine the sizes of the vectors so that dimensions match, depending on the
  BLAS operation. For operations on square matrices, only `N` is given.
* `alpha` and `beta` are scalars.
* `mA` is a container for a column-major matrix `A`.
* `lda` is the leading dimension of `mA`, i.e the step between an element and
  its neighbor in the next column and same row. `lda` must be at least `M`.
* `vx` and `vy` are containers for vectors `x` and `y`.
* `incx` and `incy` are their increments (cf BLAS 1).

| operation | arguments | description |
|---|---|---|
| `_gemv` | `ex`, `trans`, `M`, `N`, `alpha`, `mA`, `lda`, `vx`, `incx`, `beta`, `vy`, `incy`  | Generalised matrix-vector product followed by a vector sum: `y = alpha * A * x + beta * y`. *Note: the dimensions of the vectors depend on the transpose mode (`x`: `N` and `y`: `M` for mode `'n'` ; `x`: `M` and `y`: `N` otherwise)* |
| `_trmv`  | `ex`, `uplo`, `trans`, `diag`, `N`, `alpha`, `mA`, `lda`, `vx`, `incx` | Matrix-vector product for a triangular matrix: `x = A * x` |
| `_symv` | `ex`, `uplo`, `N`, `alpha`, `mA`, `lda`, `vx`, `incx`, `beta`, `vy`, `incy` | Variant of GEMV for a symmetric matrix (`y = alpha * A * x + beta * y`). *Note: `uplo` specifies which side of the matrix will be read* |
| `_ger` | `ex`, `M`, `N`, `alpha`, `vx`, `incx`, `vy`, `incy`, `mA`, `lda` | Generalised vector-vector product followed by a matrix sum: `A = alpha * x * yT + A` |
| `_syr` | `ex`, `uplo`, `N`, `alpha`, `vx`, `incx`, `mA`, `lda` | Generalised vector squaring followed by a sum with a symmetric matrix: `A = alpha * x * xT + A` |
| `_syr2` | `ex`, `uplo`, `N`, `alpha`, `vx`, `incx`, `vy`, `incy`, `mA`, `lda` | Generalised vector products followed by a sum with a symmetric matrix: `A = alpha*x*yT + alpha*y*xT + A` |

### BLAS 3

The following table sums up the interface that can be found in
[blas3_interface.h](include/interface/blas3_interface.h).

For all these operations:

* `A`, `B` and `C` are containers for the column-major matrices A, B and C.
* `lda`, `ldb` and `ldc` are the leading dimensions of the matrices A, B and C
  (cf BLAS 2). The leading dimension of a matrix must be greater than or equal
  to its number of rows.
* `transa` and `transb` are the transpose modes of the matrices A and B
  (cf BLAS 2).
* `M`, `N` and `K` are the dimensions of the matrices. The dimensions
  **after transposition** are A: `M`x`K`, B: `K`x`N`, C: `M`x`N`.
* `alpha` and `beta` are scalars.
* `batch_size` is an integer.

| operation | arguments | description |
|---|---|---|
| `_gemm` | `ex`, `transa`, `transb`, `M`, `N`, `K`, `alpha`, `A`, `lda`, `B`, `ldb`, `beta`, `C`, `ldc` | Generalised matrix-matrix multiplication followed by matrix addition: `C = alpha * A * B + beta * C` |
| `_gemm_batched` | `ex`, `transa`, `transb`, `M`, `N`, `K`, `alpha`, `A`, `lda`, `B`, `ldb`, `beta`, `C`, `ldc`, `batch_size` | Same as `_gemm` but the containers contain `batch_size` end-to-end matrices. GEMM operations are performed independently with matching matrices. |

## Tests and benchmarks

The tests and benchmarks have their own documentation:

- [Documentation of the tests](test/README.md)
- [Documentation of the benchmarks](benchmark/README.md)

## CMake options

CMake options are given using `-D` immediately followed by the option name, the
symbol `=` and a value (`ON` and `OFF` can be used for boolean options and are
equivalent to 1 and 0). Example: `-DBLAS_ENABLE_TESTING=OFF`

Some of the supported options are:

| name | value | description |
|---|---|---|
| `BLAS_ENABLE_TESTING` | `ON`/`OFF` | Set it to `OFF` to avoid building the tests (`ON` is the default value) |
| `BLAS_ENABLE_BENCHMARK` | `ON`/`OFF` | Set it to `OFF` to avoid building the benchmarks (`ON` is the default value) |
| `TARGET` | name | By default SYCL-BLAS library is built for CPU. Use that flag to compile it for a specific backend (**highly recommended** for performance). The supported targets are: `INTEL_GPU`, `AMD_GPU`, `ARM_GPU`, `RCAR` |
| `CMAKE_PREFIX_PATH` | path | List of paths to check when searching for dependencies |
| `CMAKE_INSTALL_PREFIX` | path | Specify the install location, used when invoking `ninja install` |
| `BLAS_ENABLE_STATIC_LIBRARY` | `ON`/`OFF` | Build as a static library (`OFF` by default) |
| `ENABLE_EXPRESSION_TESTS` | `ON`/`OFF` | Build additional tests that use the header-only framework (e.g to test expression trees); `OFF` by default |
| `BLAS_VERIFY_BENCHMARK` | `ON`/`OFF` | Verify the results of the benchmarks instead of only measuring the performance. See the documentation of the benchmarks for more details. `ON` by default |


## Cross-Compile

To cross-compile SYCL-BLAS first the following environment variables must be
set:

```bash
export COMPUTECPP_TOOLCHAIN_DIR="PATH TO TOOLCHAIN_DIR"
export COMPUTECPP_TARGET_TRIPLE="PATH TO TARGET_TRIPLE"
export COMPUTECPP_SYSROOT_DIR="$PATH TO SYSROOT_DIR"
```
The following CMake command can be used to cross-compile SYCL-BLAS:

```bash
cmake  -GNinja                                                                                           \
    ${SOURCE_ROOT}                                                                                       \
   -DCMAKE_PREFIX_PATH="${OPENBLAS_PATH}"                                                                 \
   -DComputeCpp_DIR="${COMPUTECPP_DEVICE_PATH}"                                                          \
   -DComputeCpp_HOST_DIR="${COMPUTECPP_X86_PATH}"                                                        \
   -DCMAKE_TOOLCHAIN_FILE="${SYCL_BLAS_PATH}/external/computecpp-sdk/cmake/toolchains/gcc-generic.cmake" \
   -DCMAKE_BUILD_TYPE='Release'                                                                          \
   -DCMAKE_INSTALL_PREFIX=${CROSS_COMPILED_SYCLBLAS_INSTALL}                                             \
   -DOpenCL_INCLUDE_DIR="${OpenCL_Headers_PATH}"                                                         \
   -DOpenCL_LIBRARY="${OpenCL_LIBRARY}"                                                                  \
   -DCOMPUTECPP_BITCODE="${DEVICE_BITCODE}"                                                              \
   -DCMAKE_CXX_FLAGS='-O3'                                                                               \
   -DTARGET="${CHOSEN_TARGET}"
```


## Tests and benchmarks

The tests and benchmarks have their own documentation:

- [Documentation of the tests](test/README.md)
- [Documentation of the benchmarks](benchmark/README.md)

## Contributions

Please see the [CONTRIBUTING file](CONTRIBUTING.md) for further details if you would like to
contribute code, build systems, bug fixes or similar.